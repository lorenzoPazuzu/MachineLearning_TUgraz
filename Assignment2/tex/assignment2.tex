\documentclass[english]{exercisesheet}

\usepackage{bm}
\usepackage{mymath}
\usepackage{graphicx}

\author{Lorenzo Minneci, Daniel Strenger}
\immatriculationnumber{11939539, 01531211}
\semester{SS 2020}
\subject{Machine Learning}
\sheetnumber{2}

 \begin{document}
 \makedocumentheader
  \begin{nexercise}{Online Bayesian Linear Regression}
  \begin{solution} 1.
  
  \begin{align*}
  p(t|\bm{x}, \bm{w}) = \prod_{n=1}^N \sqrt{\frac{\beta}{2\pi}}\exp{(-\beta \frac{( \bm{t}-\bm{w}^{T} \bm{\phi}(x_n))^{2}}{2})}
  \end{align*}
  \begin{align*}
      \log(p(\bm{t}|\bm{x}, \bm{w})) = \sum_{n=1}^N \frac{1}{2}(\ln{\beta}-\ln{2\pi}) + [-\frac{\beta}{2}(\bm{t}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}] = \frac{N}{2}(\ln\beta-\ln2\pi)-\beta\underbrace{\frac{1}{2}\sum_{n=1}^N(\bm{t}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}}_{E_D}= 
  \end{align*}
  \begin{align*} \frac{N}{2}(\ln\beta-\ln2\pi)-\beta E_{D}
  \end{align*}
  \end{solution}
  
  \par 
  \begin{solution} 2.
  \\ It's a multivariate Gaussian, and again we are going to compute the logarithm. So:
  \begin{align*}
  \ln(p(\bm{w})) = \ln(\mathcal{N}(\bm {w} | \bm {0}, \alpha^{-1}I)) =  \ln\left(\frac{1}{(2\pi)^{\frac{M+1}{2}}|\alpha^{-1}I|}\exp\left((-\frac{1}{2}\bm{w}^{T}(\alpha^{-1}I)\bm{w})\right) \right)= \\ -\frac{M+1}{2}\ln(2\pi)-\frac{1}{2}\ln(|\alpha^{-1}I|)-\left(\frac{1}{2}\bm{w}^{T}(\alpha^{-1}I)\bm{w}\right) = \\ -\frac{M+1}{2}\ln(2\pi)+\frac{M+1}{2}\ln(\alpha)-\frac{1}{2}\sum_{i=1}^M+1 \frac{w_i}{\alpha}
  \end{align*}
  In this setting we notice that only the third term is dependent on $w$, while the first two terms as constants. We'll use this consideration for the next exercise.
    \end{solution}\\
    \begin{solution} 3.
  \par 
  Bayes theorem states that:
  \begin{align*}
  p(\bm{w}|\bm{t}) = \frac{p(\bm{t}|\bm{w})p(\bm{w})}{p(\bm{t})}
  \end{align*}
  We assume that $p(\bm t)$ is not depending on $w$, so also its logarithm will be independent. \\ Let's compute the logarithm of the posterior simply as:
  \begin{align*}
      \ln(p(\bm{t}|\bm{w}))+ \ln(p(\bm{w})) + cost = \underbrace{\frac{N}{2}\left(\ln\beta-\ln(2\pi)\right)}_\text{cost} - \frac{\beta}{2}\sum_{n=1}^{N}(\bm{t}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2} + \\ + \left[\underbrace{-\frac{D}{2}\ln(2\pi)-\frac{D}{2}\ln(\alpha)}_\text{cost}+\left(-\frac{1}{2}\bm{w}^{T}(\alpha^{-1}I)^{-1}\bm{w}\right)\right] = \\
      \end{align*}
      concluding:
     \begin{align*}
      \ln(p(\bm{w}|\bm{t})) = -\frac{\beta}{2}\sum_{n=1}^{N}(\bm{t}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}-\frac{\alpha}{2}\bm{w}^{T}\bm{w} + cost
    \end{align*}
    where we grouped in $cost$ all the terms constant with respect to $w$. The alpha becomes a scalar multiplying the vector product.
    The maximization of this distribution w.r.t. $\bm{w}$ is equivalent to the minimization of the sum-of-squares approach with addition of a quadratic regularization term $\lambda = \frac{\alpha}{\beta}$. \\
      \end{solution}
      \par
      \begin{solution} 4.\\
      Let's now maximise the log-posterior w.r.t. $w$
      We can express the vectors $\phi(x_{i})$ as rows of the design matrix $\Phi$
      \begin{align*}
          \frac{\partial}{\partial w_{i}}\left[\frac{\beta}{2}\sum_{n=1}^{N}(\bm{t}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}-\frac{\alpha}{2}\bm{w}^{T}\bm{w}+cost \right] = \frac{\beta}{2}(2\Phi_{i}^{T}\Phi \bm{w} - 2\Phi_{i}^{T}\bm{t}) + \frac{\alpha}{2}2\bm{w} = \\ = \left(\beta\Phi^{T}\Phi+\alpha I\right)\bm{w}-\beta\Phi_{i}^{T}\bm{t} \stackrel{!}{=} 0 \\
      \end{align*}
      \begin{align*}
          \Rightarrow \bm{w}_{map} = (\Phi^{T}\Phi + \frac{\alpha}{\beta}I)^{-1}\Phi \bm{t} \\
      \end{align*}
      That we can see it's equal to $\bm{m} = \beta\bm{S\Phi^{T}t}$, being $\bm{S}=\left(\alpha \bm{I} +\beta \bm{\Phi^{T}\Phi}\right)^{-1}$.\\ We took $\beta$ out of $\bm{S}$, which is finally a scalar multiplier in $\bm{m}$ .\\
      \end{solution}
      \par
      \begin{solution} 5, 6, 7, 8.\\
            Here $K = 33$ was chosen.
              We can see here that as we increase sigma the curve gets smoother while interpolating between the data points. On the other hand, if we decrease sigma (e.g. $\sigma_{r}^{2} = 0.1$) the curve present peaks, sometimes showing a discontinuity on the first derivative of the curve. This is because our function learns very particular “behaviour” from the data: each data points has a narrow effect on the prediction of the next value.
            We can also notice how abruptly our prediction for $x > K$ flattens on the mean value.
        \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{1.pdf}
        \includegraphics[width=1\textwidth]{2.pdf}
        \caption{Effect of different $\sigma_{r}^{2}$ on the prediction curve}
        \end{figure}
        \cleardoublepage
       As we can see from the bottom Fig. 2, the regression curve fits quite decently the data, but the uncertainty (green area) increases as soon as we try to predict a big number of data points. It's also not surprising the smaller values of $\sigma$ (so a smaller green area) in the region in which the data points lie. Also here the regression curve tents pretty quickly to the mean value.
       As the number of the data point points increases, the uncertainty region gets smaller with respect to the data range.
        \begin{figure}
        \includegraphics[width=0.8\textwidth]{3.pdf}
        \includegraphics[width=0.8\textwidth]{4.pdf}
        \caption{Prediction P=20 days for K=10 data samples}
        
        \end{figure}
        \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{5.pdf}
        \caption{K = 25}
        \includegraphics[width=1\textwidth]{6.pdf}
        \caption{K = N = 50}
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=8cm]{7.pdf}
        \includegraphics[width=1\textwidth, height=8cm]{8.pdf}
        \includegraphics[width=1\textwidth, height=8cm]{9.pdf}
        \caption{Complete predictive distribution and MAP solution}
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=8cm]{10.pdf}
        \includegraphics[width=1\textwidth, height=8cm]{11.pdf}
        \includegraphics[width=1\textwidth, height=8cm]{12.pdf}
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=8cm]{13.pdf}
        \end{figure}
      \end{solution}
 \end{nexercise}
\cleardoublepage
 \begin{nexercise}{Logistic Regression}
\end{nexercise}

\begin{solution}
 1. We have $p(t=1|\bm{\tilde{x}})=\sigma(\bm{\tilde{w}}^t\bm{\tilde{x}})=\sigma(1\cdot\bm{w}^t\bm{\tilde{x}})$ and

 \begin{align*}
 p(t=-1|\bm{\tilde{x}})=1-\sigma(\bm{\tilde{w}}^t\bm{\tilde{x}})=1-\frac{1}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}=\frac{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}-\frac{1}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}\\=\frac{e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}=\frac{1}{e^{\bm{\tilde{w}}^t\bm{\tilde{x}}}+1}=\sigma(-\bm{\tilde{w}}^t\bm{\tilde{x}})
\end{align*}\\
\par 2.
\begin{align*}
 -\log (p(\bm{\tilde{w}}|\bm{t},\bm{x}))=-\log(p(\bm{w}))-\sum_{n=1}^N \log(\sigma(t_n\bm{\tilde{w}^t\tilde{x}_n}))=\\-\log(p(\bm{w}))-\sum_{n=1}^N \log(\frac{1}{1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}})\\
 =-\log(p(\bm{w}))+\sum_{n=1}^N \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})=\\-\log\left(\frac{1}{(2\pi S^{2})^{N/2}}\exp\left(-\frac{1}{2S^{2}}\sum_{n=1}^N w_n^2\right)\right)+\sum_{n=1}^N \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})\\
 =-\log\left(\frac{1}{(2\pi S^{2})^{N/2}}\right)+\frac{1}{2S^{2}}\sum_{n=1}^N w_n^2+\sum_{n=1}^N \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})
\end{align*}\\
\par 3.
\begin{align*}
 \frac{\partial}{\partial b}E(\bm{\tilde{w}})=\sum_{n=1}^N\frac{\partial}{\partial b} \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})=\sum_{n=1}^N\frac{-t_ne^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}}{1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}},\\
 \frac{\partial}{\partial w_i}E(\bm{\tilde{w}})=\frac{1}{2S^{2}}\sum_{n=1}^N  \frac{\partial}{\partial w_i} w_n^2+\sum_{n=1}^N\frac{\partial}{\partial w_i} \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})\\
 =\frac{w_i}{S^{2}}+\sum_{n=1}^N\frac{-t_n\tilde{x}_n^ie^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}}{1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}}
\end{align*}
\\
\par 5. b)
\begin{figure}[h]
\centering
 \includegraphics[width=0.7\textwidth]{points.pdf}
 \caption{The generated points}
\end{figure}

\par e,f) The decision boundary is the curve
\begin{align*}
 p(t=1|\bm{x})=0.5 \Leftrightarrow \frac{1}{1+e^{\bm{\tilde{w}^t\tilde{x}}}}=0.5\Leftrightarrow 1+e^{\bm{\tilde{w}^t\tilde{x}}}=2\Leftrightarrow \bm{\tilde{w}^t\tilde{x}}=\log(1)=0,
\end{align*}
so it is the line
\begin{equation*}
 y=-\frac{w_1}{w_2}x-\frac{b}{w_2}.
\end{equation*}

\begin{figure}
 \includegraphics[width=0.5\textwidth]{simulated-1.pdf}
 \includegraphics[width=0.5\textwidth]{simulated-8.pdf}
 \caption{decision boundary and probabilities for $S^2=10^{-3},10^4$}
\end{figure}
In this case the computed decision boundary really seperates the red and blue training points (for both values of $S^2$), although in some cases the points may not be linearly separable in two dimensions, which also happened in some tests. The probability around the decision boundary is about 0.5, which is clear, because the boundary was chosen for this reason. If a high variance for $\bm{w}$ is assumed, the probability decreases rather slowly when moving away from the decision boundary. For higher $S^2$ the probability decreases much faster and the predictions can be made with more certainty.\\
\par 6. The accuracy for variing $S^2$ is
\begin{table}[h]
\begin{tabular}{lll}
 $S^2$&training  &validation\\ \hline
 $10^{-4}$& 0.6088 & 0.6029 \\
 $10^{-3}$& 0.8435 & 0.8377 \\
 $10^{-2}$& 0.9003 & 0.8826 \\
 $10^{-1}$& 0.9193 & 0.9123 \\
 1& 0.9165 & 0.9152 \\
 $10^{1}$& 0.9169 & 0.9109 \\
 $10^{2}$& 0.9165 & 0.9145 \\
 $10^{3}$& 0.9143 & 0.9101 \\
 $10^{4}$& 0.9165 & 0.9138
\end{tabular}
\end{table}\\
It fits the previous observation that for higher $S^2$ the predictions can be made with more certainty, but this effect seems to be limited by $S^2 \approx 0.1$, after that no real improvement is achieved.
\end{solution}

 
\end{document}
