\documentclass[english]{exercisesheet}

\usepackage{bm}
\usepackage{mymath}
\usepackage{graphicx}

\author{Lorenzo Minneci, Daniel Strenger}
\immatriculationnumber{11939539, 01531211}
\semester{SS 2020}
\subject{Machine Learning}
\sheetnumber{2}

 \begin{document}
 \makedocumentheader
  \begin{nexercise}{Online Bayesian Linear Regression}
  \begin{solution} 1.
  
  \begin{align*}
  p(t|\bm{x}, \bm{w}) = \prod_{n=1}^N \sqrt{\frac{\beta}{2\pi}}\exp{(-\beta \frac{(t_{n}-\bm{w}^{T} \bm{\phi}(x_n))^{2}}{2})}
  \end{align*}
  \begin{align*}
      \log(p(\bm{t}|\bm{x}, \bm{w})) = \sum_{n=1}^N \frac{1}{2}(\ln{\beta}-\ln{2\pi}) + [-\frac{\beta}{2}(t_{n}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}] = \frac{N}{2}(\ln\beta-\ln2\pi)-\beta\underbrace{\frac{1}{2}\sum_{n=1}^N(t_{n}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}}_{E_D}= 
  \end{align*}
  \begin{align*} \frac{N}{2}(\ln\beta-\ln2\pi)-\beta E_{D}
  \end{align*}
  \end{solution}
  
  \par 
  \begin{solution} 2.
  \\ It's a multivariate Gaussian, and again we are going to compute the logarithm. So:
  \begin{align*}
  \ln(p(\bm{w})) = \ln(\mathcal{N}(\bm {w} | \bm {0}, \alpha^{-1}I)) =  \ln\left(\frac{1}{(2\pi)^{\frac{M+1}{2}}|\alpha^{-1}I|}\exp\left((-\frac{1}{2}\bm{w}^{T}(\alpha^{-1}I)\bm{w})\right) \right)= \\ -\frac{M+1}{2}\ln(2\pi)-\frac{1}{2}\ln(|\alpha^{-1}I|)-\left(\frac{1}{2}\bm{w}^{T}(\alpha^{-1}I)\bm{w}\right) = \\ -\frac{M+1}{2}\ln(2\pi)+\frac{M+1}{2}\ln(\alpha)-\frac{\alpha}{2}\bm{w^{T}w}
  \end{align*}
  In this setting we notice that only the third term is dependent on $w$, while the first two terms are constant. We'll use this consideration for the next exercise.
    \end{solution}\\
    \begin{solution} 3.
  \par 
  Bayes theorem states that:
  \begin{align*}
  p(\bm{w}|\bm{t}) = \frac{p(\bm{t}|\bm{w})p(\bm{w})}{p(\bm{t})}
  \end{align*}
  We assume that $p(\bm t)$ is not depending on $w$, so also its logarithm will be independent. \\ Let's compute the logarithm of the posterior simply as:
  \begin{align*}
      \ln(p(\bm{t}|\bm{w}))+ \ln(p(\bm{w})) + cost = \underbrace{\frac{N}{2}\left(\ln\beta-\ln(2\pi)\right)}_\text{cost} - \frac{\beta}{2}\sum_{n=1}^{N}(t_{n}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2} + \\ + \left[\underbrace{-\frac{M+1}{2}\ln(2\pi)-\frac{M+1}{2}\ln(\alpha)}_\text{cost}+\left(-\frac{\alpha}{2}\bm{w^{T}w}\right)\right] = \\
      \end{align*}
      concluding:
     \begin{align*}
      \ln(p(\bm{w}|\bm{t})) = -\frac{\beta}{2}\sum_{n=1}^{N}(t_{n}-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}-\frac{\alpha}{2}\bm{w}^{T}\bm{w} + cost
    \end{align*}
    where we grouped in $cost$ all the terms constant with respect to $w$.
    The maximization of this distribution w.r.t. $\bm{w}$ is equivalent to the minimization of the sum-of-squares approach with addition of a quadratic regularization term $\lambda = \frac{\alpha}{\beta}$. \\
      \end{solution}
      \par
      \begin{solution} 4.\\
      Let's now maximise the log-posterior w.r.t. $w$.
      We will express the elements t and $\phi(x_{i})$ as part of $\bm{t}$ and $\bm{\Phi}$
      \begin{align*}
          \frac{\partial}{\partial w_{i}}\left[\frac{\beta}{2}\sum_{n=1}^{N}(t-\bm{w}^{T}\bm{\phi}(x_{n}))^{2}-\frac{\alpha}{2}\bm{w}^{T}\bm{w}+cost \right] = \frac{\beta}{2}(2\Phi_{i}^{T}\Phi \bm{w} - 2\Phi_{i}^{T}\bm{t}) + \frac{\alpha}{2}2\bm{w} = \\ = \left(\beta\Phi^{T}\Phi+\alpha I\right)\bm{w}-\beta\Phi_{i}^{T}\bm{t} \stackrel{!}{=} 0 \\
      \end{align*}
      \begin{align*}
          \Rightarrow \bm{w}_{map} = (\Phi^{T}\Phi + \frac{\alpha}{\beta}I)^{-1}\Phi \bm{t} \\
      \end{align*}
      That we can see it's equal to $\bm{m} = \beta\bm{S\Phi^{T}t}$, being $\bm{S}=\left(\alpha \bm{I} +\beta \bm{\Phi^{T}\Phi}\right)^{-1}$.\\ We took $\beta$ out of $\bm{S}$, which is finally a scalar multiplier in $\bm{m}$ .\\
      \end{solution}
      \par
      \begin{solution} 5, 6, 7, 8.\\
            In Fig. 1 $K = 33$ was chosen,
            $\sigma_{r}^{2} = 6$, $\alpha = 0.0005$, $\beta = 0.0009$.
              We can see here that as we increase $\sigma_{r}^{2}$ the curve gets slightly smoother while interpolating between the data points. On the other hand, given the same parameters, if we decrease $\sigma_{r}^{2}$ (e.g. $\sigma_{r}^{2} = 0.1$) the curve present peaks, since we're assigning a high likelihood to each data sample. We're then facing overfitting, and our function learns very particular behaviours from our data and try to fits it in a over complicated model.
              In this case, each data sample influences very slightly its neighbours.
            We can also notice how abruptly our prediction for $x > K$ flattens on the mean value.
        \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{1.pdf}
        \caption{$\sigma_{r}^{2} = 6$}
        \includegraphics[width=1\textwidth]{2.pdf}
        \caption{$\sigma_{r}^{2} = 0.1$}
        \end{figure}
        \cleardoublepage
       As we can see from Fig. 3, the regression curve for K=10 fits quite decently the data, but the uncertainty increases as soon as we try to predict a big of data points compared to the data we have. It's also not surprising that we have smaller values of $\sigma$ in the region in which the data points lie.
       As the number of the data point points increases (Fig. 4, 5), the green $+/-2\sigma$ region gets smaller with respect to the data range.\\
       In Fig. 6 we highlighted the MAP weights as well.
       In Fig. 7, 8 setting $\alpha = 0$ we obtained an unregularized least-squares plot, since $\lambda = \frac{\alpha}{\beta}$.
       In Fig. 9, 10, we set different (extreme) ratios for $\frac{\alpha}{\beta}$, with $0.1 <\alpha, \beta < 10$
       In this last two Figures we notice how different values for of $\frac{\alpha}{\beta}$ encourages the coefficients to decay.
       Finally, in Figure 11 we see how decreasing $\beta$ (precision parameter) increases $\sigma$.
       
        \begin{figure}
        \includegraphics[width=0.8\textwidth]{3.pdf}
        \includegraphics[width=0.8\textwidth]{4.pdf}
        \caption{Prediction P=20 days for K=10 data samples}
        
        \end{figure}
        \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{5.pdf}
        \caption{K = 25}
        \includegraphics[width=1\textwidth]{6.pdf}
        \caption{K = N = 50}
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=8cm]{7.pdf}
        \includegraphics[width=1\textwidth, height=8cm]{8.pdf}
        \includegraphics[width=1\textwidth, height=8cm]{9.pdf}
        \caption{Complete predictive distribution and MAP solution}
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=8cm]{10.pdf}
        \caption{$\alpha = 0$, $\sigma_{r}^{2}=5$}
        \includegraphics[width=1\textwidth, height=8cm]{11.pdf}
        \caption{$\alpha = 0$, $\sigma_{r}^{2}=10$}
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=8cm]{12.pdf}
        \caption{$\frac{\alpha}{\beta}=100$, $0.1 <\alpha, \beta < 10$ }
        \includegraphics[width=1\textwidth, height=8cm]{13.pdf}
        \caption{$\frac{\alpha}{\beta}=0.01$, $0.1 <\alpha, \beta < 10$ }
        \end{figure}
        \cleardoublepage
        \begin{figure}
        \includegraphics[width=1\textwidth, height=7cm]{15.pdf}
        \includegraphics[width=1\textwidth, height=7cm]{16.pdf}
        \includegraphics[width=1\textwidth, height=7cm]{14.pdf}
        \caption{$\frac{\alpha}{\beta}=1$ for decreasing values of $\alpha$ or $\beta$}
        \end{figure}
      \end{solution}
 \end{nexercise}
\cleardoublepage
 \begin{nexercise}{Logistic Regression}
\end{nexercise}

\begin{solution}
 1. We have $p(t=1|\bm{\tilde{x}})=\sigma(\bm{\tilde{w}}^t\bm{\tilde{x}})=\sigma(1\cdot\bm{w}^t\bm{\tilde{x}})$ and

 \begin{align*}
 p(t=-1|\bm{\tilde{x}})=1-\sigma(\bm{\tilde{w}}^t\bm{\tilde{x}})=1-\frac{1}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}=\frac{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}-\frac{1}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}\\=\frac{e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}{1+e^{-\bm{\tilde{w}}^t\bm{\tilde{x}}}}=\frac{1}{e^{\bm{\tilde{w}}^t\bm{\tilde{x}}}+1}=\sigma(-\bm{\tilde{w}}^t\bm{\tilde{x}})
\end{align*}\\
\par 2.
\begin{align*}
 -\log (p(\bm{\tilde{w}}|\bm{t},\bm{x}))=-\log(p(\bm{w}))-\sum_{n=1}^N \log(\sigma(t_n\bm{\tilde{w}^t\tilde{x}_n}))=\\-\log(p(\bm{w}))-\sum_{n=1}^N \log(\frac{1}{1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}})\\
 =-\log(p(\bm{w}))+\sum_{n=1}^N \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})=\\-\log\left(\frac{1}{(2\pi S^{2})^{N/2}}\exp\left(-\frac{1}{2S^{2}}\sum_{n=1}^N w_n^2\right)\right)+\sum_{n=1}^N \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})\\
 =-\log\left(\frac{1}{(2\pi S^{2})^{N/2}}\right)+\frac{1}{2S^{2}}\sum_{n=1}^N w_n^2+\sum_{n=1}^N \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})
\end{align*}\\
\par 3.
\begin{align*}
 \frac{\partial}{\partial b}E(\bm{\tilde{w}})=\sum_{n=1}^N\frac{\partial}{\partial b} \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})=\sum_{n=1}^N\frac{-t_ne^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}}{1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}},\\
 \frac{\partial}{\partial w_i}E(\bm{\tilde{w}})=\frac{1}{2S^{2}}\sum_{n=1}^N  \frac{\partial}{\partial w_i} w_n^2+\sum_{n=1}^N\frac{\partial}{\partial w_i} \log(1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}})\\
 =\frac{w_i}{S^{2}}+\sum_{n=1}^N\frac{-t_n\tilde{x}_n^ie^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}}{1+e^{-t_n\bm{\tilde{w}^t\tilde{x}_n}}}
\end{align*}
\\
\par 5. b)
\begin{figure}[h]
\centering
 \includegraphics[width=0.7\textwidth]{points.pdf}
 \caption{The generated points}
\end{figure}

\par e,f) The decision boundary is the curve
\begin{align*}
 p(t=1|\bm{x})=0.5 \Leftrightarrow \frac{1}{1+e^{\bm{\tilde{w}^t\tilde{x}}}}=0.5\Leftrightarrow 1+e^{\bm{\tilde{w}^t\tilde{x}}}=2\Leftrightarrow \bm{\tilde{w}^t\tilde{x}}=\log(1)=0,
\end{align*}
so it is the line
\begin{equation*}
 y=-\frac{w_1}{w_2}x-\frac{b}{w_2}.
\end{equation*}

\begin{figure}
 \includegraphics[width=0.5\textwidth]{simulated-1.pdf}
 \includegraphics[width=0.5\textwidth]{simulated-8.pdf}
 \caption{decision boundary and probabilities for $S^2=10^{-3},10^4$}
\end{figure}
In this case the computed decision boundary really seperates the red and blue training points (for both values of $S^2$), although in some cases the points may not be linearly separable in two dimensions, which also happened in some tests. The probability around the decision boundary is about 0.5, which is clear, because the boundary was chosen for this reason. If a high variance for $\bm{w}$ is assumed, the probability decreases rather slowly when moving away from the decision boundary. For higher $S^2$ the probability decreases much faster and the predictions can be made with more certainty.\\
\par 6. The accuracy for variing $S^2$ is
\begin{table}[h]
\begin{tabular}{lll}
 $S^2$&training  &validation\\ \hline
 $10^{-4}$& 0.6088 & 0.6029 \\
 $10^{-3}$& 0.8435 & 0.8377 \\
 $10^{-2}$& 0.9003 & 0.8826 \\
 $10^{-1}$& 0.9193 & 0.9123 \\
 1& 0.9165 & 0.9152 \\
 $10^{1}$& 0.9169 & 0.9109 \\
 $10^{2}$& 0.9165 & 0.9145 \\
 $10^{3}$& 0.9143 & 0.9101 \\
 $10^{4}$& 0.9165 & 0.9138
\end{tabular}
\end{table}\\
It fits the previous observation that for higher $S^2$ the predictions can be made with more certainty, but this effect seems to be limited by $S^2 \approx 0.1$, after that no real improvement is achieved.
\end{solution}

 
\end{document}
